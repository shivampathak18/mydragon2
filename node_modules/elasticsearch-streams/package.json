{
  "name": "elasticsearch-streams",
  "description": "Stream in and out of Elasticsearch",
  "version": "0.0.9",
  "repository": {
    "type": "git",
    "url": "https://github.com/hmalphettes/elasticsearch-streams.git"
  },
  "main": "index.js",
  "scripts": {
    "test": "jshint lib/*.js test/*.js && mocha"
  },
  "author": {
    "name": "Hugues Malphettes"
  },
  "license": "MIT",
  "keywords": [
    "elasticsearch",
    "stream"
  ],
  "dependencies": {},
  "devDependencies": {
    "chai": "*",
    "elasticsearch": "*",
    "random-document-stream": "*"
  },
  "readme": "# Elasticsearch streams\n\nExpose a Writeable stream for bulk commands and a Readable stream from\nhits and documents responses.\n\nUse case: pipe to and from levelup, pouchdb and other friends.\n\nThe client that executes the requests is wrapped in a closure.\nIt is expected to provide the Elasticsearch reponse's body as a JSON.\n\nSee the examples and tests with the official Elasticsearch-js client.\n\n# Examples:\n\n## Stream random records into Elasticsearch\n```\nvar WritableBulk = require('elasticsearch-streams').WritableBulk;\nvar client = new require('elasticsearch').Client();\n\nvar bulkExec = function(bulkCmds, callback) {\n  client.bulk({\n    index : 'myindex',\n    type  : 'mytype',\n    body  : bulkCmds\n  }, callback);\n};\nvar ws = new WritableBulk(bulkExec);\nvar toBulk = new TransformToBulk(function getIndexTypeId(doc) { return { _id: doc.id }; });\n// stream 42 random records into ES\nrequire('random-document-stream')(42).pipe(toBulk).pipe(ws).on('finish', done);\n```\n\nNOTE: One must listen to the `close` event emitted by the write stream to know\nwhen all the data has been written and flushed to Elasticsearch.\n\nListening to `finish` does not mean much really as we are in this situation:\nhttps://github.com/joyent/node/issues/5315#issuecomment-16670354\n\nFor example to close the ES client as soon as we are done:\n\n```\nws.on('close', function () {\n  client.close();\n});\n```\n\n## Stream search results from Elasticsearch\n```\nvar ReadableSearch = require('elasticsearch-streams').ReadableSearch;\nvar client = new require('elasticsearch').Client();\n\nvar searchExec = function searchExec(from, callback) {\n  client.search({\n    index: 'myindex',\n    from: from,\n    size: 12,\n    body: {\n      query: { match_all: {} }\n    }\n  }, callback);\n};\n\nvar rs = new ReadableSearch(searchExec);\nvar ws = new require('stream').Writable({objectMode:true});\nws._write = function(chunk, enc, next) {\n  console.log('a hit', hit);\n  next();\n};\n\nrs.pipe(ws).on('close', done);\n```\n\nIf we want to start the stream at an offset and define a limit:\n\n```\nvar offset = 7;\nvar limit  = 21;\nvar page   = 12;\n\nvar searchExec = function searchExec(from, callback) {\n  client.search({\n    index: 'myindex',\n    from: from + offset,\n    size: (offset + from + page) > limit ? (limit - offset - from) : page,\n    body: {\n      query: { match_all: {} }\n    }\n  }, callback);\n};\n```\n\n## Stream scroll/scan results from Elasticsearch\n```\nvar scrollExec = function scrollExec(from, callback) {\n  if (this.scroll_id) {\n    return client.scroll({\n      scrollId : this.scroll_id,\n      scroll   : '30s'\n    }, callback);\n  }\n  // get a scroll id first\n  var self = this;\n  client.search({\n    index: 'myindex',\n    scroll: '20s',\n    size: 42,\n    body: {\n      query: { match_all: {} }\n    }\n  }, function(e, resp) {\n    self.scroll_id = resp._scroll_id;\n    callback(e, resp);\n  });\n};\nrs = new ReadableSearch(scrollExec);\n```\n\n## Stream IDs into Elasticsearch multi-get and get documents out.\n```\nvar mgetExec = function(docs, callback) {\n  client.mget({\n    index: 'myindex',\n    type: 'mytype',\n    body: {\n      docs: { ids: docs }\n    }\n  }, callback);\n};\nts = new PipableDocs(mgetExec, 4);\n\n// Naive read stream of 12 ids that are numbers\nvar rs = new require('stream').Readable({objectMode: true});\nrs._read = function() {\n  for (var i = 0; i < 12; i++) {\n    rs.push(i);\n  }\n  rs.push(null);\n};\n\nvar ws = new require('stream').Writable({objectMode:true});\nws._write = function(chunk, enc, next) {\n  console.log(hit._id + ' found: ' + hit._found, hit);\n  next();\n};\n\nrs.pipe(ts).pipe(ws).on('finish', onFinish);\n```\n\n# LICENSE\nelasticsearch-streams is freely distributable under the terms of the MIT license.\n\nCopyright (c) 2015 Sutoiku, Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\ndocumentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\nrights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit\npersons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the\nSoftware.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\nWARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\nOTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/hmalphettes/elasticsearch-streams/issues"
  },
  "homepage": "https://github.com/hmalphettes/elasticsearch-streams",
  "_id": "elasticsearch-streams@0.0.9",
  "_shasum": "9aafcc556d4a3eaece667164d602438c0c4d62e8",
  "_from": "https://registry.npmjs.org/elasticsearch-streams/-/elasticsearch-streams-0.0.9.tgz",
  "_resolved": "https://registry.npmjs.org/elasticsearch-streams/-/elasticsearch-streams-0.0.9.tgz"
}
